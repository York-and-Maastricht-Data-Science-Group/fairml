[% import 'Util.eol'; %]
# load machine learning libraries
import numpy as np
import pandas as pd
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
# from sklearn.svm import LinearSVC
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline

# load AI Fair 360 libraries
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.metrics import ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing
from aif360.explainers import MetricTextExplainer
from aif360.datasets import StandardDataset

# load IPython libraries
# from IPython.display import *

#load matplotlib
import matplotlib.pyplot as plt

print("========================")
print("FairML: [%=fairml.name%]")
print("========================")
print("Description:")
print("[%=fairml.description%]")

[% for (biasMitigation in fairml.biasMitigations) { %]
print("")
print("========================")
print("Bias Mitigation: [%=biasMitigation.name%]")
print("------------------------")
[% var biasChecking = biasMitigation.biasChecking; %]
resource = "[%=biasChecking.dataset.path%]"
protected_attributes = ['[%=biasChecking.protectedAttribute%]']
predicted_attribute = '[%=biasChecking.predictedAttribute%]'
favorable_classes = ['[%=biasChecking.favorableClass%]']
dropped_attributes = [[%=biasChecking.droppedAttributes.listToLine()%]]
training_size = [%=biasChecking.trainValidationTestSplit[0]%] * 1.0 
validation_size = [%=biasChecking.trainValidationTestSplit[1]%] * 1.0 
test_size = [%=biasChecking.trainValidationTestSplit[2]%] * 1.0 
total_size = training_size + validation_size + test_size
priviledgedGroup = [%=biasChecking.priviledgedGroup%] 
unprivilegedGroup = [%=biasChecking.unpriviledgedGroup%]

# load dataset
data = pd.read_csv(resource, header=0)

dataset_original = StandardDataset(df=data, label_name=predicted_attribute, favorable_classes=favorable_classes,
                protected_attribute_names=protected_attributes,
                privileged_classes=[[priviledgedGroup]],
                instance_weights_name=None,
                # categorical_features=feature_cols,
                features_to_keep=[],
                features_to_drop=dropped_attributes,
                na_values=[], 
                custom_preprocessing=None,
                metadata=None)
                
dataset_original_train, dataset_original_validation, dataset_original_test = dataset_original.split([training_size/total_size, (training_size + validation_size) / total_size], shuffle=True)
#
privileged_groups = [{protected_attributes[0] : priviledgedGroup}]
unprivileged_groups = [{protected_attributes[0] : unprivilegedGroup}]

print("")
print("Original Bias Checking: [%=biasChecking.name%]")
print("-------------")
print("Dataset: [%=biasChecking.dataset.name%]")
    [% for (checkingMethod in biasChecking.checkingMethods) { %]
    [% if (checkingMethod.name = "BinaryLabelDatasetMetric") { %]
print("Preprocessing Metrics")
metric_original_train = [%=checkingMethod.name%](dataset_original_train, 
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
        [% for (explainer in checkingMethod.explainers) { %]
explainer_train = [%=explainer.name%](metric_original_train)
            [% for (func in explainer.functions) { %]
            
print("Original explainer: " + explainer_train.[%=func.name%]())
            [% } %]
        [% } %]
        [% for (func in checkingMethod.functions) { %]
print("Original [%=func.name%]: %f" % metric_original_train.[%=func.name%]())
        [% } %]
        [% } %]
    [% } %] 

    [% for (trainingMethod in biasChecking.trainingMethods) { %]
print("")
print("Original Training: [%=trainingMethod.type.name%], Parameters: [%=trainingMethod.parameters.listToLineWithoutQuote()%]")
print("-------------")


classifier = [%=trainingMethod.type.name%]([%=trainingMethod.parameters.listToLineWithoutQuote()%])
model = make_pipeline(StandardScaler(),
                      classifier)
fit_params = {'[%=trainingMethod.type.name.toLowerCase()%]__sample_weight': dataset_original_train.instance_weights}
model_original_train = model.fit(dataset_original_train.features, dataset_original_train.labels.ravel(), **fit_params)

y_pred = model_original_train.predict(dataset_original_test.features)
y_test = dataset_original_test.labels.ravel()

# Model Accuracy, how often is the classifier correct?
original_accuracy = metrics.accuracy_score(y_test, y_pred)
print("Original Accuracy:", original_accuracy)

if isinstance(classifier, DecisionTreeClassifier): 
	plt.figure(figsize=(12, 5), dpi=500)
	tree.plot_tree(classifier,
	               feature_names=dataset_original_train.feature_names,
	               class_names=["[%=biasChecking.unpriviledgedGroup%]", "[%=biasChecking.priviledgedGroup%]"],
	               filled=True,
	               rounded=True);
	plt.savefig('graphics/Original-[%=trainingMethod.type.name%]_[%=trainingMethod.parameters.listToLineWithoutQuote().replace("'","").replace(",", "-").replace(" ", "")%].png')
    [% } %]

    [% for (checkingMethod in biasChecking.checkingMethods) { %]
    [%   if (checkingMethod.name == "ClassificationMetric") { %] 
print("In-processing Metrics")
dataset_original_train_pred = dataset_original_train.copy()
y_val_pred = model_original_train.predict(dataset_original_train.features)
dataset_original_train_pred.labels = y_val_pred
metric_original_train = [%=checkingMethod.name%](dataset_original_train, 
                                                dataset_original_train_pred,
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
        [% for (explainer in checkingMethod.explainers) { %]
explainer_train = [%=explainer.name%](metric_original_train)
            [% for (func in explainer.functions) { %]
            
print("Original explainer: " + explainer_train.[%=func.name%]())
            [% } %]
        [% } %]
        [% for (func in checkingMethod.functions) { %]
print("Original [%=func.name%]: %f" % metric_original_train.[%=func.name%]())
        [% } %]
        [% } %]
    [% } %] 
print("")
print("Bias Mitigation")
print("-------------")
print("Method: [%=biasMitigation.mitigationMethod.name%]")
mitigation_method = [%=biasMitigation.mitigationMethod.name%](unprivileged_groups=unprivileged_groups,
                privileged_groups=privileged_groups)
dataset_mitigated_train = mitigation_method.fit_transform(dataset_original_train)
dataset_mitigated_test = mitigation_method.fit_transform(dataset_original_test)
 
print("")
print("After Mitigation Bias Checking: [%=biasChecking.name%]")
print("-------------")
print("Dataset: [%=biasChecking.dataset.name%]")
    [% for (checkingMethod in biasChecking.checkingMethods) { %]
dataset_mitigated_train_pred = dataset_mitigated_train.copy()
y_val_pred = model_original_train.predict(dataset_mitigated_train.features)
dataset_mitigated_train_pred.labels = y_val_pred
metric_mitigated_train = [%=checkingMethod.name%](dataset_mitigated_train,
                                             dataset_mitigated_train_pred, 
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
        [% for (explainer in checkingMethod.explainers) { %]
explainer_train = [%=explainer.name%](metric_mitigated_train)
            [% for (func in explainer.functions) { %]
            
print("After mitigation explainer: " + explainer_train.[%=func.name%]())
            [% } %]
        [% } %]
        [% for (func in checkingMethod.functions) { %]
print("After mitigation [%=func.name%]: %f" % metric_mitigated_train.[%=func.name%]())
        [% } %]
    [% } %]
    [% for (trainingMethod in biasChecking.trainingMethods) { %]

print("")
print("After Mitigation Training: [%=trainingMethod.type.name%], Parameters: [%=trainingMethod.parameters.listToLineWithoutQuote()%]")
print("-------------")

classifier = [%=trainingMethod.type.name%]([%=trainingMethod.parameters.listToLineWithoutQuote()%])
model = make_pipeline(StandardScaler(),
                      classifier)
fit_params = {'[%=trainingMethod.type.name.toLowerCase()%]__sample_weight': dataset_mitigated_train.instance_weights}
model_mitigated_train = model.fit(dataset_mitigated_train.features, dataset_mitigated_train.labels.ravel(), **fit_params)

y_pred = model_mitigated_train.predict(dataset_mitigated_test.features)
y_test = dataset_mitigated_test.labels.ravel()

# Model Accuracy, how often is the classifier correct?
after_mitigation_accuracy = metrics.accuracy_score(y_test, y_pred)
print("After Mitigation Accuracy:", after_mitigation_accuracy)

if isinstance(classifier, DecisionTreeClassifier): 
	plt.figure(figsize=(12, 5), dpi=500)
	tree.plot_tree(classifier,
	               feature_names=dataset_mitigated_train.feature_names,
	               class_names=["[%=biasChecking.unpriviledgedGroup%]", "[%=biasChecking.priviledgedGroup%]"],
	               filled=True,
	               rounded=True);
	plt.savefig('graphics/Mitigated-[%=trainingMethod.type.name%]_[%=trainingMethod.parameters.listToLineWithoutQuote().replace("'","").replace(",", "-").replace(" ", "")%].png')
    [% } %]

[% } %]
