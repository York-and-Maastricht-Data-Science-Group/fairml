[% import 'Util.eol'; %]
[%var number1 = 0; %]
[%var number2 = 0; %]
[%var number3 = 0; %]
# # FairML: [%=fairml.name%]
# [%=fairml.description%]

# Load dependencies.
import numpy as np
import pandas as pd
from sklearn import tree
from sklearn.tree import DecisionTreeClassifier
# from sklearn.svm import LinearSVC
# from sklearn.neighbors import KNeighborsClassifier
# from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import LogisticRegression
from sklearn import metrics
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from aif360.metrics import BinaryLabelDatasetMetric
from aif360.metrics import ClassificationMetric
from aif360.algorithms.preprocessing import Reweighing
from aif360.explainers import MetricTextExplainer
from aif360.datasets import StandardDataset
import matplotlib.pyplot as plt
from collections import defaultdict

print("========================")
print("FairML: [%=fairml.name%]")
print("========================")
print("Description:")
print("[%=fairml.description%]")

results = []
[% if (fairml.biasMitigations.size() > 0) { %]
line_num = pd.Series(range(1,([%=fairml.biasMitigations.size()%]*2) + 1))
[% } %]

[% for (biasMitigation in fairml.biasMitigations) { %]
# ## [%=number1++%]. Bias Mitigation: [%=biasMitigation.name%]
print("")
print("========================")
print("Bias Mitigation: [%=biasMitigation.name%]")
print("------------------------")
[% var biasChecking = biasMitigation.biasChecking; %]
resource = "[%=biasChecking.dataset.path%]"
protected_attributes = ['[%=biasChecking.protectedAttribute%]']
predicted_attribute = '[%=biasChecking.predictedAttribute%]'
favorable_classes = ['[%=biasChecking.favorableClass%]']
dropped_attributes = [[%=biasChecking.droppedAttributes.listToLine()%]]
training_size = [%=biasChecking.trainValidationTestSplit[0]%]  
validation_size = [%=biasChecking.trainValidationTestSplit[1]%]  
test_size = [%=biasChecking.trainValidationTestSplit[2]%] 
total_size = (training_size + validation_size + test_size) * 1.0
priviledgedGroup = [%=biasChecking.priviledgedGroup%] 
unprivilegedGroup = [%=biasChecking.unpriviledgedGroup%]
mitigation_results = defaultdict(list)
results.append(mitigation_results)
mitigation_results["Mitigation"].append("Original")

# Load dataset.
data = pd.read_csv(resource, header=0)
dataset_original = StandardDataset(df=data, label_name=predicted_attribute, favorable_classes=favorable_classes,
                protected_attribute_names=protected_attributes,
                privileged_classes=[[priviledgedGroup]],
                instance_weights_name=None,
                # categorical_features=feature_cols,
                features_to_keep=[],
                features_to_drop=dropped_attributes,
                na_values=[], 
                custom_preprocessing=None,
                metadata=None)
dataset_original_train, dataset_original_validation, dataset_original_test = dataset_original.split([training_size/total_size, (training_size + validation_size) / total_size], shuffle=True)
privileged_groups = [{protected_attributes[0] : priviledgedGroup}]
unprivileged_groups = [{protected_attributes[0] : unprivilegedGroup}]

# ### [%=number1%].[%=number2++%]. Original Bias Checking: [%=biasChecking.name%]

print("")
print("Original Bias Checking: [%=biasChecking.name%]")
print("-------------")
print("Dataset: [%=biasChecking.dataset.name%]")
mitigation_results["Dataset"].append("[%=biasChecking.dataset.name%] ("+str(training_size)+":"+str(validation_size)+":"+str(test_size)+")")

# #### [%=number1%].[%=number2%].[%=number3++%]. Preprocessing Metrics

print("Preprocessing Metrics")
    [% for (checkingMethod in biasChecking.checkingMethods) { %]
    [% if (checkingMethod.name = "BinaryLabelDatasetMetric") { %]
metric_original_train = [%=checkingMethod.name%](dataset_original_train, 
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
        [% for (explainer in checkingMethod.explainers) { %]
explainer_train = [%=explainer.name%](metric_original_train)
            [% for (func in explainer.functions) { %]
            
print("Original explainer: " + explainer_train.[%=func.name%]())
            [% } %]
        [% for (func in checkingMethod.functions) { %]
print("Original [%=func.name%]: %f" % metric_original_train.[%=func.name%]())
        [% } %]
        [% } %]
    [% } %] 
    [% } %]

    [% for (trainingMethod in biasChecking.trainingMethods) { %]

# #### [%=number1%].[%=number2%].[%=number3++%]. Original Training: [%=trainingMethod.type.name%], Parameters: [%=trainingMethod.parameters.listToLineWithoutQuote()%]"

print("")
print("Original Training: [%=trainingMethod.type.name%], Parameters: [%=trainingMethod.parameters.listToLineWithoutQuote()%]")
print("-------------")
mitigation_results["Classifier"].append("[%=trainingMethod.type.name%]")

# Train the model from the original train data
classifier = [%=trainingMethod.type.name%]([%=trainingMethod.parameters.listToLineWithoutQuote()%])
model = make_pipeline(StandardScaler(), classifier)
fit_params = {'[%=trainingMethod.type.name.toLowerCase()%]__sample_weight': dataset_original_train.instance_weights}
model_original_train = model.fit(dataset_original_train.features, dataset_original_train.labels.ravel(), **fit_params)

# #### [%=number1%].[%=number2%].[%=number3++%]. Check the Accuracy of the Prediction

print("Check the accuracy of the prediction")
y_pred = model_original_train.predict(dataset_original_test.features)
y_test = dataset_original_test.labels.ravel()
original_accuracy = metrics.accuracy_score(y_test, y_pred)
print("Original Accuracy:", original_accuracy)
mitigation_results["Accuracy"].append(original_accuracy)

        [% if (trainingMethod.type.name == "DecisionTreeClassifier") { %]
if isinstance(classifier, DecisionTreeClassifier): 
	plt.figure(figsize=(12, 5), dpi=500)
	tree.plot_tree(classifier,
	               feature_names=dataset_original_train.feature_names,
	               class_names=["[%=biasChecking.unpriviledgedGroup%]", "[%=biasChecking.priviledgedGroup%]"],
	               filled=True,
	               rounded=True);
	plt.savefig('graphics/Original-[%=trainingMethod.type.name%]_[%=trainingMethod.parameters.listToLineWithoutQuote().replace("'","").replace(",", "-").replace(" ", "")%].png')
        [% } %]
    [% } %]

# #### [%=number1%].[%=number2%].[%=number3++%]. Inprocessing Metrics
    [% for (checkingMethod in biasChecking.checkingMethods) { %]
    [%   if (checkingMethod.name == "ClassificationMetric") { %] 
print("In-processing Metrics")
dataset_original_train_pred = dataset_original_train.copy()
y_val_pred = model_original_train.predict(dataset_original_train.features)
dataset_original_train_pred.labels = y_val_pred
metric_original_train = [%=checkingMethod.name%](dataset_original_train, 
                                                dataset_original_train_pred,
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
        [% for (explainer in checkingMethod.explainers) { %]
explainer_train = [%=explainer.name%](metric_original_train)
            [% for (func in explainer.functions) { %]
            
print("Original explainer: " + explainer_train.[%=func.name%]())
            [% } %]
        [% } %]
        [% for (func in checkingMethod.functions) { %]
print("Original [%=func.name%]: %f" % metric_original_train.[%=func.name%]())
mitigation_results["[%=func.name%]"].append(metric_original_train.[%=func.name%]())
        [% } %]
        [% } %]
    [% } %]

# ### [%=number1%].[%=number2++%]. Bias Mitigation: Method: [%=biasMitigation.mitigationMethod.name%]  
[% number3=0; %]

print("")
print("Bias Mitigation")
print("-------------")
print("Method: [%=biasMitigation.mitigationMethod.name%]")
mitigation_results = defaultdict(list)
results.append(mitigation_results)
mitigation_results["Mitigation"].append("[%=biasMitigation.mitigationMethod.name%]")

mitigation_method = [%=biasMitigation.mitigationMethod.name%](unprivileged_groups=unprivileged_groups,
                privileged_groups=privileged_groups)
dataset_mitigated_train = mitigation_method.fit_transform(dataset_original_train)
dataset_mitigated_test = mitigation_method.fit_transform(dataset_original_test)

# ### [%=number1%].[%=number2++%]. After-mitigation Bias Checking: [%=biasChecking.name%]
[% number3=0; %]

print("")
print("After-mitigation Bias Checking: [%=biasChecking.name%]")
print("-------------")
print("Dataset: [%=biasChecking.dataset.name%]")
mitigation_results["Dataset"].append("[%=biasChecking.dataset.name%] ("+str(training_size)+":"+str(validation_size)+":"+str(test_size)+")")

# #### [%=number1%].[%=number2%].[%=number3++%]. After-mitigation Preprocessing Metrics

print("Preprocessing Metrics")
    [% for (checkingMethod in biasChecking.checkingMethods) { %]
    [% if (checkingMethod.name = "BinaryLabelDatasetMetric") { %]
metric_mitigated_train = [%=checkingMethod.name%](dataset_mitigated_train, 
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
        [% for (explainer in checkingMethod.explainers) { %]
explainer_train = [%=explainer.name%](metric_mitigated_train)
            [% for (func in explainer.functions) { %]
            
print("Original explainer: " + explainer_train.[%=func.name%]())
            [% } %]
        [% for (func in checkingMethod.functions) { %]
print("Original [%=func.name%]: %f" % metric_mitigated_train.[%=func.name%]())
mitigation_results["[%=func.name%]"].append(metric_mitigated_train.[%=func.name%]())
        [% } %]
        [% } %]
    [% } %] 
    [% } %]


    [% for (trainingMethod in biasChecking.trainingMethods) { %]

# #### [%=number1%].[%=number2%].[%=number3++%]. After Mitigation Training: [%=trainingMethod.type.name%], Parameters: [%=trainingMethod.parameters.listToLineWithoutQuote()%]
print("")
print("After Mitigation Training: [%=trainingMethod.type.name%], Parameters: [%=trainingMethod.parameters.listToLineWithoutQuote()%]")
print("-------------")
mitigation_results["Classifier"].append("[%=trainingMethod.type.name%]")

# Train the model from the after-mitigation train data
classifier = [%=trainingMethod.type.name%]([%=trainingMethod.parameters.listToLineWithoutQuote()%])
fit_params = {'[%=trainingMethod.type.name.toLowerCase()%]__sample_weight': dataset_mitigated_train.instance_weights}
model_mitigated_train = model.fit(dataset_mitigated_train.features, dataset_mitigated_train.labels.ravel(), **fit_params)

# #### [%=number1%].[%=number2%].[%=number3++%]. Check the Accuracy of the Prediction

print("Check the accuracy of the prediction")
y_pred = model_mitigated_train.predict(dataset_mitigated_test.features)
y_test = dataset_mitigated_test.labels.ravel()
after_mitigation_accuracy = metrics.accuracy_score(y_test, y_pred)
print("After Mitigation Accuracy:", after_mitigation_accuracy)
mitigation_results["Accuracy"].append(after_mitigation_accuracy)

# #### [%=number1%].[%=number2%].[%=number3++%]. Inprocessing Metrics

print("Inprocessing Metrics")
    [% for (checkingMethod in biasChecking.checkingMethods) { %]
dataset_mitigated_train_pred = dataset_mitigated_train.copy()
y_val_pred = model_mitigated_train.predict(dataset_mitigated_train.features)
dataset_mitigated_train_pred.labels = y_val_pred
metric_mitigated_train = [%=checkingMethod.name%](dataset_mitigated_train,
                                             dataset_mitigated_train_pred, 
                                             unprivileged_groups=unprivileged_groups,
                                             privileged_groups=privileged_groups)
        [% for (explainer in checkingMethod.explainers) { %]
explainer_train = [%=explainer.name%](metric_mitigated_train)
            [% for (func in explainer.functions) { %]
            
print("After mitigation explainer: " + explainer_train.[%=func.name%]())
            [% } %]
        [% } %]
        [% for (func in checkingMethod.functions) { %]
print("After mitigation [%=func.name%]: %f" % metric_mitigated_train.[%=func.name%]())
mitigation_results["[%=func.name%]"].append(metric_mitigated_train.[%=func.name%]())
        [% } %]
    [% } %]
        [% if (trainingMethod.type.name == "DecisionTreeClassifier") { %]
if isinstance(classifier, DecisionTreeClassifier): 
	plt.figure(figsize=(12, 5), dpi=500)
	tree.plot_tree(classifier,
	               feature_names=dataset_mitigated_train.feature_names,
	               class_names=["[%=biasChecking.unpriviledgedGroup%]", "[%=biasChecking.priviledgedGroup%]"],
	               filled=True,
	               rounded=True);
	plt.savefig('graphics/Mitigated-[%=trainingMethod.type.name%]_[%=trainingMethod.parameters.listToLineWithoutQuote().replace("'","").replace(",", "-").replace(" ", "")%].png')
	   [% } %]
    [% } %]
[% } %]

# ## Summary 
print("")
table = pd.concat([pd.DataFrame(m) for m in results], axis=0).set_axis(line_num)
print(table)

pd.concat([pd.DataFrame(m) for m in results], axis=0).set_axis(line_num)

[% //fairml.p2j(); %]